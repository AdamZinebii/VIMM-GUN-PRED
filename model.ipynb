{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee90b189bdff4faba8e62d830e520307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/6.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7118e2e9afd74e1ea8935c1e3a7265ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/375M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed713526f76413a9bf6002ea0ff1954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for cuda devices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m raw_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(requests\u001b[38;5;241m.\u001b[39mget(img_url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mraw)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m input_points \u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m450\u001b[39m, \u001b[38;5;241m600\u001b[39m]]] \u001b[38;5;66;03m# 2D localization of a window\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     13\u001b[0m masks \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpost_process_masks(outputs\u001b[38;5;241m.\u001b[39mpred_masks\u001b[38;5;241m.\u001b[39mcpu(), inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshaped_input_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:231\u001b[0m, in \u001b[0;36mBatchFeature.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# check if v is a floating point\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(v):\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# cast and send to device\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         new_data[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for cuda devices"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "input_points = [[[450, 600]]] # 2D localization of a window\n",
    "#inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model(**inputs)\n",
    "masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, TimeDistributed, GlobalAveragePooling2D, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n_samples = 1000\n",
    "n_frames = 4\n",
    "frame_height, frame_width = 224, 224\n",
    "input_shape = (n_frames, frame_height, frame_width, 3)\n",
    "\n",
    "# Dummy dataset generation (replace with actual data loading)\n",
    "# X is shape (1000, 4, 224, 224, 3), y is shape (1000,)\n",
    "X = np.random.rand(n_samples, n_frames, frame_height, frame_width, 3).astype(np.float32)\n",
    "y = np.random.randint(2, size=(n_samples,))\n",
    "\n",
    "# SAM: Shift-and-Mix function\n",
    "def shift_and_mix(features, shifts=4):\n",
    "    shifted_features = []\n",
    "    for i in range(shifts):\n",
    "        shifted = tf.roll(features, shift=i, axis=1)\n",
    "        shifted_features.append(shifted)\n",
    "    mixed_features = tf.reduce_mean(tf.stack(shifted_features), axis=0)\n",
    "    return mixed_features\n",
    "\n",
    "# MAMBA: Memory-Augmented Multi-step Attention function\n",
    "def memory_augmented_attention(features, memory_size=256):\n",
    "    memory = tf.Variable(tf.zeros([memory_size]), trainable=False)\n",
    "    attention_weights = tf.nn.softmax(tf.matmul(features, tf.expand_dims(memory, axis=-1)), axis=1)\n",
    "    attended_features = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "    memory.assign(attended_features)\n",
    "    return attended_features\n",
    "\n",
    "# Feature extractor using ResNet50\n",
    "cnn_base = ResNet50(weights='imagenet', include_top=False, input_shape=(frame_height, frame_width, 3))\n",
    "cnn_output = GlobalAveragePooling2D()(cnn_base.output)\n",
    "cnn_model = Model(inputs=cnn_base.input, outputs=cnn_output)\n",
    "\n",
    "# Model definition\n",
    "video_input = Input(shape=input_shape)\n",
    "\n",
    "# Apply CNN to each frame\n",
    "time_distributed_cnn = TimeDistributed(cnn_model)(video_input)\n",
    "\n",
    "# Apply SAM to the extracted features\n",
    "sam_features = Lambda(shift_and_mix)(time_distributed_cnn)\n",
    "\n",
    "\n",
    "# LSTM layer to process the sequence of frames\n",
    "lstm_output = LSTM(256, return_sequences=False)(mamba_features)\n",
    "\n",
    "# Fully connected layer\n",
    "fc = Dense(128, activation='relu')(lstm_output)\n",
    "output = Dense(1, activation='sigmoid')(fc)\n",
    "\n",
    "# Final model\n",
    "model = Model(inputs=video_input, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
